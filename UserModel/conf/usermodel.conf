# Datos de conexión a Hive
database.driver=org.apache.hive.jdbc.HiveDriver
database.type=2
database.server=192.168.1.51
database.port=10000
database.database=default
database.user=hive
database.pass=
# Directorio donde residen los serdes necesarios
serdesLib=/usr/lib/hive/lib/external/
# Tipo de origen de los datos, respecto de donde se ejecuta el aplicativo. LOCAL o REMOTE. 
# Si LOCAL, se envia información por red con API HDFS. Si REMOTE, la información esta en Linux donde reside HDFS y se accede mediante conexión SSH.
source.type=REMOTE
# Directorio de origen de los logs (Windows Local Ej. D:\\Data o Linux Remoto Ej. /home/data)
source.data=/extra/newDir/
# Ficheros de origen de tablas de lookup, servers, festivos y horarios. (Windows Local Ej. D:\\Data\\calendar.csv o Linux Remoto Ej. /home/data/calendar.csv)
source.servers=/root/Downloads/info/machines.csv
source.calendar=/root/Downloads/info/calendar.csv
source.workinghours=/root/Downloads/info/working.csv
# Directorio de destino de los datos en HDFS
hdfs.data=/user/root/
# Datos de conexión a HDFS
hdfs.user=root
hdfs.fqdn=hadoopnode01.osbs.com
hdfs.port=8020
# Datos de conexión SSH a Linux con acceso a HDFS
system.user=root
system.pass=s21.sec!
system.host=192.168.1.51
# Directorio que se va a usar para almacen de datos del modelo en Hive
hive.storage=/user/root/hive/data/


